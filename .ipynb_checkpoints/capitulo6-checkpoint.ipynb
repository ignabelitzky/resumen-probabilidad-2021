{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c08775dc",
   "metadata": {},
   "source": [
    "# *Capítulo 6 - Estimación puntual*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b712bf41",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "\n",
    "Dado un parametro de interés, tal como la media $/mu$ o la proporción $p$ de una población, el objetivo de la estimación puntual es utilizar una muestra para calcular un número que representa en cierto sentido una buena suposición del valor verdadero del parámetro. El número resultante se llama *estimación puntual.* En la sección 6.2 se describen e ilustran dos métodos importantes para obtener estimaciones puntuales: el método de momentos y el método de máxima probabilidad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919efcc3",
   "metadata": {},
   "source": [
    "## 6.1 Algunos conceptos generales de estimación puntual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cae1b0b",
   "metadata": {},
   "source": [
    "**Definición:** Una **estimación puntual** de un parámetro $\\theta$ es un número único que puede ser considerado como un valor sensible de $\\theta$. Se obtiene una estimación puntual seleccionando un estadístico apropiado y calculando su valor con los datos muestrales dados. El estadístico seleccionado se llama **estimador puntual** de $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0877f359",
   "metadata": {},
   "source": [
    "### Estimadores insesgados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c4b536",
   "metadata": {},
   "source": [
    "**Definición:** Se dice que un estimador puntual $\\hat{\\theta}$ es un **estimador insesgado** de $\\theta$ si $E(\\hat{\\theta}) = \\theta$ para todo valor posible de $\\theta$. Si $\\hat{\\theta}$ es insesgado, la diferencia $E(\\hat{\\theta}) - \\theta$ se conoce como el **sesgo** de $\\hat{\\theta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a18b4c9",
   "metadata": {},
   "source": [
    "---\n",
    "Es decir, $\\hat{\\theta}$ es sesgado si su distribución de probabilidad (es decir, muestreo) siempre esta \"centrada\" en el valor verdadero del parámetro. Supóngase que $\\hat{\\theta}$ es un estimador insesgado; entonces si $\\theta = 100$, la distribución muestral $\\hat{\\theta}$ esta centrada en 100; si $\\theta = 27.5$, en ese caso la distribución muestral $\\hat{\\theta}$ esta centrada en 27.5, y asi sucesivamente. La siguiente figura ilustra la distribución de varios estimadores sesgados e insesgados. Observese que \"centrada\" en este caso significa que el valor esperado, no la mediana, de la distribución de $\\hat{\\theta}$ es igual a $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabde7f7",
   "metadata": {},
   "source": [
    "![Image 1](resources/imgs_cap6/img_cap6_01.png)\n",
    "$$\\text{Funciones de densidad de probabilidad de un estimador sesgado $\\hat{\\theta}_{1}$ y un estimador insesgado $\\hat{\\theta}_{2}$, de un parámetro $\\theta$}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbb0d12",
   "metadata": {},
   "source": [
    "**Proposición:** Cuando $X$ es una variable aleatoria binomial con parámetros $n$ y $p$, la proporción muestral $\\hat{p} = X/n$ es un estimador sesgado de $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7154d7af",
   "metadata": {},
   "source": [
    "**Principio de estimación insesgada**  \n",
    "Cuando se elige entre varios estimadores diferentes de $\\theta$, se elige uno insesgado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8e3c7a",
   "metadata": {},
   "source": [
    "**Proposición:** Sea $X_{1},\\ X_{2}, \\ldots,\\ X_{n}$ una muestra aleatoria de una distribución con media $\\mu$ y varianza $\\sigma^{2}$. Entonces el estimador\n",
    "$$\\hat{\\sigma}^{2} = S^{2} = \\frac{\\sum(X_{i} - \\overline{X})^{2}}{n -1}$$\n",
    "es un estimador insesgado de $\\sigma^{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9c4a10",
   "metadata": {},
   "source": [
    "**Demostracion:**\n",
    "Para cualquier variable aleatoria $Y,\\ V(Y) = E(Y^{2}) - [E(Y)]^{2}$, por lo tanto $E(Y^{2}) = V(Y) + [E(Y)]^{2}$. Aplicando esto a\n",
    "$$ S^{2} = \\frac{1}{n-1} \\left[\\sum X_{i}^{2} - \\frac{(\\sum X_{i})^{2}}{n}\\right] $$\n",
    "se obtiene\n",
    "\\begin{align*}\n",
    "    E(S^{2}) &= \\frac{1}{n-1} \\left(\\sum E(X_{i}^{2}) - \\frac{1}{n} E[(\\sum X_{i})^{2}] \\right) \\\\\n",
    "    &= \\frac{1}{n-1} \\left( \\sum(\\sigma^{2} + \\mu^{2}) - \\frac {1}{n} \\{V(\\sum X_{i}) + [E(\\sum X_{i})]^{2}\\} \\right) \\\\\n",
    "    &= \\frac{1}{n-1} \\left( n \\sigma^{2} + n \\mu^{2} - \\frac{1}{n} n \\sigma^{2} - \\frac{1}{n} (n \\mu)^{2} \\right) \\\\\n",
    "    &= \\frac{1}{n-1} \\{ n \\sigma^{2} - \\sigma^{2} \\} = \\sigma^{2}\\ \\text{(como se desea)}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3b32fa",
   "metadata": {},
   "source": [
    "---\n",
    "**Proposición:** Si $X_{1},\\ X_{2}, \\ldots,\\ X_{n}$ es una muestra aleatoria tomada de una distribución con media $\\mu$, entonces $\\overline{X}$ es un estimador sesgado de $\\mu$. Si además la distribución es continua y simétrica, entonces $\\tilde{X}$ y cualquier media recortada también son estimadores insesgados de $\\mu$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4977464",
   "metadata": {},
   "source": [
    "### Estimadores con varianza mínima"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc2b71b",
   "metadata": {},
   "source": [
    "**Principio de estimación insesgada con varianza mínima.**  \n",
    "Entre todos los estimadores de $\\theta$ insesgados, se selecciona el de varianza mínima. El $\\hat{\\theta}$ resultante se llama **estimador insesgado con varianza mínima (EIVM)** de $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03582ac0",
   "metadata": {},
   "source": [
    "---\n",
    "La siguiente figura ilustra las funciones de densidad de probablidad de los dos estimadores insesgados, donde $\\hat{\\theta}_{1}$ tiene una varianza más pequeña que $\\hat{\\theta}_{2}$. Entonces es mas probable que $\\hat{\\theta}_{1}$ produzca una estimación próxima al valor verdadero $\\theta$ que $\\hat{\\theta}_{2}$. El estimador insesgado con varianza mínima es, en cierto sentido, el que tiene mas probabildiades entre todos los estimadores insesgados de producir una estimación cercana al verdadero $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48b78a1",
   "metadata": {},
   "source": [
    "![Image 2](resources/imgs_cap6/img_cap6_02.png)\n",
    "$$ \\text{Gráficas de las funciones de densidad de probabilidad de dos estimadores insesgados diferentes.} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d065ebf",
   "metadata": {},
   "source": [
    "**Teorema:** Sea $X_{1},\\ldots,\\ X_{n}$ una muestra aleatoria tomada de una distribución normal con parámetros $\\mu$ y $\\sigma$. Entonces el estimador $\\hat{\\mu} = \\overline{X}$ es el estimador insesgado con varianza mínima para $\\mu$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da13f7c0",
   "metadata": {},
   "source": [
    "### Algunas complicaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917e5e8f",
   "metadata": {},
   "source": [
    "El último teorema no dice que al estimar la media $\\mu$ de una población, se deberá utilizar el estimador $\\overline{X}$ independientemente de la distribución que se esta muestreando."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3592869",
   "metadata": {},
   "source": [
    "El mejor estimador de $\\mu$ depende crucialmente de que distribución esta siendo muestreada. En particular,\n",
    "1. Si la muestra aleatoria proviene de una distribución normal, en ese caso $\\overline{X}$ es el mejor de los cuatro estimadores, puesto que tiene una varianza mínima entre todos los estimadores insesgados.\n",
    "2. Si la muestra aleatoria proviene de una distribución de Cauchy, entonces $\\overline{X}$ y $\\overline{X}_{e}$ son estimadores terribles de $\\mu$, en tanto que $\\tilde{X}$ es bastante bueno (es estimador insesgado con varianza mínima no es conocido); $\\overline{X}$ es malo porque es muy sensible a las observaciones subyacentes y las colas gruesas de la distribución de Cauchy hacen que sea improbable que aparezcan tales observaciones en cualquier muestra.\n",
    "3. Si la distribución subyacente es uniforme, el mejor estimador es $\\overline{X}_{e}$; este estimador esta influido en gran medida por las observaciones subyacentes, pero la carencia de colas hace que tales observaciones sean imposibles.\n",
    "4. En ninguna de estas tres situaciones es mejor la media recortada pero funciona razonablemente bien en las tres. Es decir, $\\overline{X}_{rec(10)}$ no sufre demasiado en comparación con el mejor procedimiento en cualquier de las tres situaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82178642",
   "metadata": {},
   "source": [
    "### Reporte de una estimación puntual: el error estándar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188ae5e2",
   "metadata": {},
   "source": [
    "**Definición:** El **error estándar** de una parámetro $\\hat{\\theta}$ es su desviación estándar $\\sigma_{\\hat{\\theta}} = \\sqrt{V(\\hat{\\theta})}$. Este es la magnitud de una desviación típica o representativa entre una estimación y el valor de $\\theta$. Si el error estándar implica parámetros desconocidos cuyos valores pueden ser estimados, la sustitución de estas estimaciones en $\\sigma_{\\hat{\\theta}}$ da el **error estándar estimado** (desviación estandar estimada) del estimador. El error estándar estimado puede ser denotado por $\\hat{\\sigma}_{\\hat{\\theta}}$ (el $\\hat{}$ sobre $\\sigma$ recalca que $\\sigma_{\\hat{\\theta}}$ esta siendo estimada) o por $s_{\\hat{\\theta}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1e631e",
   "metadata": {},
   "source": [
    "## 6.2 Métodos de estimación puntual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f94187",
   "metadata": {},
   "source": [
    "La definición de ausencia de sesgo no indica en general como se pueden obtener los estimadores insesgados. A continuación se discuten dos métodos \"constructivos\" para obtener estimadores puntuales: el método de momentos y el método de máxima probabilidad. Por *constructivo* se queire dar a entender que la definción general de cada tipo de estimador sugiere explícitamente como obtener el estimador en cualquier problema específico. Aun cuando se prefieren estimadores de máxima probabilidad a los de momento debido a ciertas propiedades de eficiencia, a menudo requieren significativamente mas cálculo que los estimadores de momento. En ocasiones es el caso que estos métodos dan estimadores insesgados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e79554",
   "metadata": {},
   "source": [
    "### El metodo de momentos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5054bf44",
   "metadata": {},
   "source": [
    "**Definicion:** Si $X_{1},\\ X_{2}, \\ldots,\\ X_{n}$ constituyen una muestra aleatoria proveniente de una funcion de masa de probabilidad o de una funcion de densidad de probabilidad $f(x)$. Con $k = 1,\\ 2,\\ 3, \\ldots$, el **momento *k-esimo* de la poblacion** o el **momento *k-esimo* de la distribucion $f(x)$**, es $E(X^{k})$. El **momento muestral *k-esimo*** es $(1/n) \\sum_{i=1}^{n} X_{i}^{k}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bef19b1",
   "metadata": {},
   "source": [
    "**Definicion:** Sea $X_{1},\\ X_{2}, \\ldots,\\ X_{n}$ una muestra aleatoria de una distribucion con funcion de masa de probabilidad o funcion de densidad de probabilidad $f(x;\\ \\theta_{1}, \\ldots,\\ \\theta_{m})$, donde $\\theta_{1}, \\ldots ,\\ \\theta_{m}$ son parametros cuyos valores son desconocidos. Entonces los **estimadores de momento** $\\hat{\\theta}_{i}, \\ldots,\\ \\hat{\\theta}_{m}$ se obtienen igualando los primeros *m* momentos muestrales con los primeros *m* momentos de la poblacion correspondientes y resolviendo para $\\theta_{1}, \\ldots,\\ \\theta_{m}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce3ce68",
   "metadata": {},
   "source": [
    "### Estimación de maxima probabilidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4c9047",
   "metadata": {},
   "source": [
    "**Definición:** Sean $X_{1},\\ X_{2}, \\ldots,\\ X_{n}$ que tienen una función de masa de probabilidad o una función de densidad de probabilidad\n",
    "\n",
    "\\begin{align*}\n",
    "    f(x_{1},\\ x_{2},\\ldots,\\ x_{n};\\ \\theta_{1},\\ldots,\\ \\theta_{m}) && \\text{(1)}\n",
    "\\end{align*}\n",
    "\n",
    "donde los parámetros $\\theta_{1}, \\ldots,\\ \\theta_{m}$ tienen valores desconocidos. Cuando $x_{1},\\ldots,\\ x_{n}$ son los valores muestrales observados y (1) se considera como una función de $\\theta_{1},\\ldots,\\ \\theta_{m}$, se llama **función de probabilidad**. Las estimaciones de máxima probabilidad $\\hat{\\theta}_{1},\\ldots,\\ \\hat{\\theta}_{m}$ son aquellos valores de las $\\theta_{i}$ que incrementan al máximo la función de probabilidad, de modo que\n",
    "\n",
    "$$ f(x_{1},\\ldots,\\ x_{n};\\ \\hat{\\theta}_{1},\\ldots,\\hat{\\theta}_m) \\ge f(x_{1},\\ldots,\\ x_{n};\\ \\theta_{1},\\ldots,\\ \\theta_m)\\ \\text{para todas las}\\ \\theta_{1},\\ldots,\\ \\theta_{m} $$\n",
    "\n",
    "Cuando se sustituyen las $X_{i}$ en lugar de las $x_{i}$, se obtienen los **estimadores de máxima probabilidad.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e974620c",
   "metadata": {},
   "source": [
    "### Estimación de funciones de parámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b093354",
   "metadata": {},
   "source": [
    "**Proposición: El principio de invarianza**  \n",
    "Sean $\\hat{\\theta}_{1},\\ \\hat{\\theta}_{2},\\ldots,\\ \\hat{\\theta}_{m}$ los estimadores de máxima probabilidad de los parámetros $\\theta_{1},\\ \\theta_{2},\\ldots,\\ \\theta_{m}$. Entonces el estimador de máxima probabilidad de cualquier función $h(\\theta_{1},\\ \\theta_{2},\\ldots,\\ \\theta_{m})$ de estos parámetros es la función $h(\\hat{\\theta}_{1},\\ \\hat{\\theta}_{2},\\ldots,\\ \\hat{\\theta}_{m})$ de los estimadores de máxima probabilidad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74452fde",
   "metadata": {},
   "source": [
    "### Comportamiento del estimador de máxima probabilidad con muestra grande"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64edb421",
   "metadata": {},
   "source": [
    "**Proposición:** En condiciones muy generales en relación con la distribucion conjunta de la muestra, cuando el tamaño *n* de la muestra es grande, el estimador de máxima probabilidad de cualquier parámetro $\\theta$ es aproximadamente insesgado $[E(\\hat{\\theta}) \\approx \\theta]$ y su varianza es casi tan pequeña como la que puede ser lograda por cualquier estimador. Expresado de otra manera, el estimador de máxima probabilidad $\\hat{\\theta}$ es aproximadamente el estimador insesgado con varianza mínima de $\\theta$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
